{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgAFEG/k/68JOevseL6AIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishna2592/Databricks-Certified-Data-Engineer-Associate/blob/main/spark_trials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYp_s7PyyrSL",
        "outputId": "b50b7215-da34-4f35-e7e4-eb9a60883548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark\n",
            "  Downloading spark-0.3.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.12/dist-packages (from spark) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.22.1)\n",
            "Requirement already satisfied: docstring-parser>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.17.0)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from spark) (3.1.6)\n",
            "Collecting json-repair>=0.52.4 (from spark)\n",
            "  Downloading json_repair-0.55.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: openai>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from spark) (2.15.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from spark) (2.12.3)\n",
            "Requirement already satisfied: starlette>=0.50.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.50.0)\n",
            "Requirement already satisfied: typing-extensions>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.38.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.40.0)\n",
            "Requirement already satisfied: websockets>=15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->spark) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (0.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=2.7.1->spark) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (1.0.9)\n",
            "Downloading spark-0.3.2-py3-none-any.whl (351 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.7/351.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.55.1-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: json-repair, spark\n",
            "Successfully installed json-repair-0.55.1 spark-0.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, input_file_name, try_to_timestamp, row_number, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "def deduplicate(df, spark):\n",
        "  business_keys = [\"CustomerID\",\"Amount\",\"timestamp\"]\n",
        "\n",
        "  window = (\n",
        "      Window.partitionBy(*business_keys).orderBy(col(\"timestamp\").desc())\n",
        "  )\n",
        "\n",
        "  ranked_df = df.withColumn(\"rank\", row_number().over(window))\n",
        "\n",
        "  deduplicated_df = ranked_df.filter(col(\"rank\") == 1).drop(\"rank\")\n",
        "\n",
        "  return deduplicated_df.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  spark = SparkSession.builder.appName(\"CSVToParquet\").getOrCreate()\n",
        "\n",
        "  df = spark.read.csv(\"/content/sample_spark_data.csv\", header=True, inferSchema=True) \\\n",
        "                  .withColumn(\"timestamp\", try_to_timestamp(col(\"timestamp_str\"), lit(\"dd-MM-yyyy HH:mm\"))) \\\n",
        "                  .drop(\"timestamp_str\")\n",
        "\n",
        "  df.printSchema()\n",
        "\n",
        "  deduplicate(df, spark)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdtgbqkMy4GC",
        "outputId": "382642b3-5bd8-4ed6-8a6b-b316695a6a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Amount: double (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            "\n",
            "+---+----------+------+-------------------+\n",
            "| ID|CustomerID|Amount|          timestamp|\n",
            "+---+----------+------+-------------------+\n",
            "|  1|   CUST001| 120.5|2023-01-01 10:00:00|\n",
            "|  3|   CUST001| 120.5|2023-01-01 11:00:00|\n",
            "|  4|   CUST004| 45.25|2023-01-04 09:15:00|\n",
            "|  5|   CUST005| 89.99|2023-01-05 16:20:00|\n",
            "|  6|   CUST006| 230.1|2023-01-06 12:00:00|\n",
            "|  7|   CUST007| 150.0|2023-01-07 13:10:00|\n",
            "|  8|   CUST008|670.45|2023-01-08 15:40:00|\n",
            "|  9|   CUST009| 310.3|2023-01-09 08:25:00|\n",
            "| 10|   CUST010| 99.99|2023-01-10 17:55:00|\n",
            "+---+----------+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def zero_stripe (matrix: List[List[int]]):\n",
        "\n",
        "  m, n = len(matrix), len(matrix[0])\n",
        "  zero_rows, zero_cols = set(), set()\n",
        "\n",
        "  #1st pass to get rows and cols indexes added to hash sets. what is hash map vs hash sets?\n",
        "  for r in range(m):\n",
        "    for c in range(n):\n",
        "      if matrix[r][c] == 0:\n",
        "        zero_rows.add(r)\n",
        "        zero_cols.add(c)\n",
        "\n",
        "  #2nd pass to zero stripe\n",
        "  for r in range(m):\n",
        "    for c in range(n):\n",
        "      if r in zero_rows or c in zero_cols:\n",
        "        matrix[r][c] = 0\n",
        "\n",
        "  return matrix\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "  matrix = [\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 0, 7, 8],\n",
        "    [9, 10, 11, 12],\n",
        "    [0, 14, 15, 16]\n",
        "]\n",
        "\n",
        "  print(zero_stripe(matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxFSI9wI1OvH",
        "outputId": "5f93a11f-6872-457e-c698-e324b722ebf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 3, 4], [0, 0, 0, 0], [0, 0, 11, 12], [0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dk0JwmtyVx1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "TimeComplexity Name ExampleinDSA WhenitHappens Real-LifeAnalogy\n",
        "O(1) ConstantTime\n",
        "Accessing array element, Hash map lookup, Stack push/pop\n",
        "Operation doesn't depend on input size\n",
        "Picking the first book from a shelf\n",
        "O(log n)\n",
        "Logarithmic Time\n",
        "Binary Search, Balanced BST search, Heap operations\n",
        "Input size reduces by half each step\n",
        "Finding a word in a dictionary\n",
        "O(n)\n",
        "Linear Time\n",
        "Traversing array, Linear Search, BFS/DFS in graph\n",
        "Every element needs to be checked once\n",
        "Taking attendance in a class\n",
        "O(n log n)\n",
        "Linearithmic Time\n",
        "Merge Sort, Quick Sort (avg), Heap Sort, Tree Sort\n",
        "Efficient divide-and-conquer algorithms\n",
        "Sorting names in a phonebook\n",
        "O(n²)\n",
        "Quadratic Time\n",
        "Bubble Sort, Selection Sort, Insertion Sort (worst case)\n",
        "Nested loops over entire input\n",
        "Comparing every student with every other student\n",
        "O(n³)\n",
        "Cubic Time\n",
        "Matrix multiplication\n",
        "Triple nested iterations\n",
        "Checking all seat combinations in a theater\n",
        "O(2^n)\n",
        "Exponential\n",
        "Recursive Fibonacci, Travelling Salesman\n",
        "Branching doubles\n",
        "Trying every combination of arranging and ordering every order of cards\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EP94Ir1xXJuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**\"Nested Array Explosion\"** scenario. It is the silent killer of Spark jobs because the input data looks small (one row, 50KB), but during the `explode()` transformation, that single row turns into 500,000 rows inside a single partition, causing a memory pressure spike that garbage collection cannot handle.\n",
        "\n",
        "Here is a **Simulation and Remediation Kit**. We will create the \"Killer Data,\" try to process it blindly, and then implement the **Observability & Protection Layer** you described.\n",
        "\n",
        "### The Code Artifacts\n",
        "\n",
        "I will generate a Python file containing:\n",
        "\n",
        "1. **The Data Generator:** Creates a dataset with a specific \"Data Bomb\" (one user with 100,000 nested events).\n",
        "2. **The Observability Pattern:** A `SafePipeline` class that implements row counting, array profiling, and sampling *before* execution.\n",
        "3. **The Fix:** A \"Salted Explode\" strategy that handles the massive array without crashing.\n",
        "\n",
        "### The Senior Engineer's Analysis\n",
        "\n",
        "Here is the breakdown of what makes this \"Senior\" level code versus \"Junior\" level code.\n",
        "\n",
        "#### 1. The Pre-Flight Check (`PipelineGuard`)\n",
        "\n",
        "* **Junior:** Just writes `df.select(explode(\"events\"))`. If it fails, they check logs.\n",
        "* **Senior:** Assumes the data is hostile. The `profile_array_column` function (lines 58-79) acts as a scanner.\n",
        "* **Why it matters:** It adds a minimal cost (one pass over metadata/size) to prevent a catastrophic crash. Finding `max(size)` allows you to fail fast or switch strategies dynamically.\n",
        "\n",
        "#### 2. The Isolation Strategy\n",
        "\n",
        "* **The Trap:** When you have 99% small arrays and 1% huge arrays, a standard `explode` might work *most* of the time, but occasionally a task will hang for hours or OOM.\n",
        "* **The Fix:** I explicitly split the DataFrame into `small_df` and `huge_df` (lines 104-107).\n",
        "* **Why:** This isolates the blast radius. If the huge data crashes, it doesn't take down the processing of the 99% good data.\n",
        "\n",
        "#### 3. The `repartition(100)` Magic (Line 119)\n",
        "\n",
        "This is the specific technical fix for \"Executor Lost due to OOM\" on explode.\n",
        "\n",
        "* **The Physics of the Crash:**\n",
        "* Input: `Row(User=\"Killer\", Array_Size=100,000)` fits on Partition 1 (Executor A).\n",
        "* Operation: `explode`.\n",
        "* Result: Executor A now has to hold 100,000 rows *and* the object overhead in memory. It hits the heap limit.\n",
        "\n",
        "\n",
        "* **The Solution:**\n",
        "* `huge_df.repartition(100)`: Forces a shuffle.\n",
        "* Even though it's one row coming *in*, if we had multiple killer rows, this distributes them to different executors *before* the explosion happens (or prepares the stage for downstream processing).\n",
        "* *Note:* If a single *output* row is too big, `repartition` won't help. But here, the *input* is one row, the *output* is many rows. The OOM often happens during the generation of those rows. By isolating the heavy rows, we ensure the executor doing this work isn't also burdened with the rest of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "### Databricks Monitoring Guide (How to verify this)\n",
        "\n",
        "When you run this in Databricks, here is exactly where you look:\n",
        "\n",
        "1. **Ganglia UI -> Cluster Memory:**\n",
        "* **Bad Pipeline:** You will see one node's memory spike to 100% while others are flat. Then a sudden drop (crash).\n",
        "* **Good Pipeline:** You will see a small spike during the `profile` stage, then a uniform distribution of memory usage during the `repartition/explode` stage.\n",
        "\n",
        "\n",
        "2. **Spark UI -> Stages -> Sort By \"Duration\":**\n",
        "* Look for the stage with the `explode`.\n",
        "* Click **\"Description\"** to see task details.\n",
        "* Sort tasks by **\"Shuffle Read Size\"** or **\"Output Size\"**.\n",
        "* **The Smoking Gun:** If one task has 10GB output and others have 10MB, you have found your nested array explosion.\n",
        "\n",
        "\n",
        "3. **Driver Logs (Log4j):**\n",
        "* My code prints: `WARNING: Data Skew Detected!`.\n",
        "* In Databricks, use the \"Driver Logs\" tab and search for \"WARNING\". This gives you the specific `user_id` that is causing the problem, allowing you to go back to the source system team and say \"User X is sending 100k events per batch, please fix upstream.\"\n",
        "\n",
        "This strategy is **native for Batch processing** but requires a specific architectural pattern to work in **Streaming**.\n",
        "\n",
        "Here is the breakdown of how, where, and when to apply this.\n",
        "\n",
        "### 1. Batch vs. Streaming Compatibility\n",
        "\n",
        "#### **Batch Processing (Native Fit)**\n",
        "\n",
        "* **Verdict:** **Perfect fit.**\n",
        "* **Why:** In batch jobs (e.g., Daily Ingestion), you have a finite dataset. Running an \"Action\" like `.collect()` (which my `profile_array_column` function does to get stats) takes a few seconds but saves hours of failure time.\n",
        "* **Usage:** You run this at the start of your job before the heavy transformations.\n",
        "\n",
        "#### **Streaming Processing (The Catch)**\n",
        "\n",
        "* **Verdict:** **Conditional fit (Must use `foreachBatch`).**\n",
        "* **The Problem:** You cannot run `.collect()` or calculate `max()` directly on a Streaming DataFrame. Spark will throw an error: `Queries with streaming sources must be executed with writeStream.start()`.\n",
        "* **The Solution:** You must wrap this logic inside a `foreachBatch` function.\n",
        "* Spark Structured Streaming processes data in small \"micro-batches.\"\n",
        "* Inside `foreachBatch`, the micro-batch appears as a **static** (Batch) DataFrame. You can then run the profiling logic on that specific chunk of data.\n",
        "\n",
        "\n",
        "\n",
        "**Streaming Implementation Snippet:**\n",
        "\n",
        "```python\n",
        "def process_micro_batch(df, batch_id):\n",
        "    # Inside here, 'df' is a Batch DataFrame of the last 30 seconds of data\n",
        "    # We can safely run our guard logic\n",
        "    clean_df = safe_explode_pipeline(spark, df)\n",
        "    \n",
        "    clean_df.write.format(\"delta\").mode(\"append\").save(\"/mnt/silver/events\")\n",
        "\n",
        "# The Stream Trigger\n",
        "spark.readStream.format(\"cloudFiles\").load(\"/mnt/raw\") \\\n",
        "    .writeStream \\\n",
        "    .foreachBatch(process_micro_batch) \\\n",
        "    .start()\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Where would this be used? (Architecture)\n",
        "\n",
        "This fits explicitly in the **Bronze to Silver** transition layer of a Medallion Architecture.\n",
        "\n",
        "* **The Input (Bronze):** Raw, nested JSON (e.g., from MongoDB, DynamoDB, or Application Logs).\n",
        "* **The Operation:** Flattening/Exploding. This is the most dangerous operation in Spark because it changes row cardinality (1 row  100k rows).\n",
        "* **The Output (Silver):** Clean, flat tables ready for analysis.\n",
        "\n",
        "**Real-World Scenarios:**\n",
        "\n",
        "1. **Clickstream Data:** A user session is one JSON object with an array of 5,000 clicks. You need to explode it to analyze individual clicks.\n",
        "2. **IoT Sensor Arrays:** A device sends a daily packet containing 86,400 temperature readings (one per second) in a single array.\n",
        "3. **Scraping Pipelines:** You scrape a webpage and get a list of 10,000 comments in one nested field.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How frequently should you use it?\n",
        "\n",
        "It depends on your **SLA (Service Level Agreement)** vs. **Cost**.\n",
        "\n",
        "* **High Frequency (Every Run):**\n",
        "* **Recommended for:** Batch jobs and High-Latency Streams (Trigger: 1-5 minutes).\n",
        "* **Cost:** The `.collect()` profiling adds overhead (e.g., 5-10 seconds per batch). If your job runs once a day, 10 seconds is nothing. If your stream runs every 10 seconds, adding 10 seconds of latency is doubling your processing time.\n",
        "\n",
        "\n",
        "* **Blind Protection (Skip Profiling):**\n",
        "* **Recommended for:** Ultra-Low Latency Streams (< 1 second).\n",
        "* **Strategy:** Don't profile. Just **assume** the data is skewed and always apply the `repartition(100)` logic before exploding.\n",
        "* **Tradeoff:** You waste a little CPU shuffling small data, but you guarantee stability without the latency \"pause\" of checking stats first."
      ],
      "metadata": {
        "id": "B7XPT9dLJGSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, explode, size, lit, max as _max, avg,\n",
        "    when, count, posexplode, expr\n",
        "    )\n",
        "from pyspark.sql.types import (\n",
        "    StructType, ArrayType, IntegerType, StructField,\n",
        "    StringType, LongType\n",
        ")\n",
        "# ==============================================================================\n",
        "# 1. THE DATA GENERATOR (Creating the \"Data Bomb\")\n",
        "# ==============================================================================\n",
        "def generate_skewed_data(spark, num_normal=1000, num_killers=5):\n",
        "    \"\"\"\n",
        "    Generates a dataset where most users have 5-10 events,\n",
        "    but a few 'killers' have 100,000 events.\n",
        "    \"\"\"\n",
        "    print(f\"Generating {num_normal} normal users and {num_killers} DATA BOMBS...\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Normal Users\n",
        "    for i in range(num_normal):\n",
        "        events = [{\"event_id\": k, \"ts\": int(time.time())} for k in range(random.randint(1, 10))]\n",
        "        data.append((f\"user_{i}\", events))\n",
        "\n",
        "    # The Killers (Simulating the deep nested JSON issue)\n",
        "    for i in range(num_killers):\n",
        "        # 100,000 events in ONE array.\n",
        "        # In memory, this is small. After explode, it's massive.\n",
        "        events = [{\"event_id\": k, \"ts\": int(time.time())} for k in range(100000)]\n",
        "        data.append((f\"KILLER_USER_{i}\", events))\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"user_id\", StringType(), True),\n",
        "        StructField(\"events\", ArrayType(\n",
        "            StructType([\n",
        "                StructField(\"event_id\", IntegerType(), True),\n",
        "                StructField(\"ts\", LongType(), True)\n",
        "            ])\n",
        "        ), True)\n",
        "    ])\n",
        "\n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. THE OBSERVABILITY LAYER (The \"Senior\" Approach)\n",
        "# ==============================================================================\n",
        "class PipelineGuard:\n",
        "  def __init__ (self, spark):\n",
        "    self.spark = spark\n",
        "    self.metrics = {}\n",
        "\n",
        "  def profile_array_column(self, df, col_name, threshold = 1000):\n",
        "    \"\"\"\n",
        "        Scans a DataFrame BEFORE expensive operations to detect array skew.\n",
        "        \"\"\"\n",
        "    print(f\"\\n----Profiling Column: {col_name}---\\n\")\n",
        "\n",
        "    #add metadata\n",
        "    df_profiled = df.withColumn(\"_array_size\", size(col(col_name)))\n",
        "\n",
        "    #calculate stats\n",
        "    stats = df_profiled.select(\n",
        "        _max(\"_array_size\").alias(\"max_size\"),\n",
        "        avg(\"_array_size\").alias(\"avg_size\"),\n",
        "        count(when(col(\"_array_size\")) > threshold, 1).alias(\"danger_rows\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    self.metrics[col_name] = stats\n",
        "\n",
        "    print(f\"Max_array_length: \", stats[\"max_size\"])\n",
        "    print(f\"avg array lenght: \", stats[\"avg_size\"])\n",
        "    print(f\"Rows exceeding threshold: {threshold}: {stats['danger_rows']}\")\n",
        "\n",
        "    if stats['danger_rows'] > 0:\n",
        "      print(\"WARNING: data skew detected! Potential OOM on explode\")\n",
        "      print(\"Sample of problematic: \")\n",
        "      df_profiiled.filter(col(\"_array_size\") > threshold) \\\n",
        "                  .select(\"user_id\", \"_array_size\").show(3)\n",
        "\n",
        "    return df_profiled\n",
        "def safe_explode_pipeline(spark,df):\n",
        "  guard = PipelineGuard(spark)\n",
        "\n",
        "  df_checked = guard.profile_array_ column(df, \"events\", threshold = 1000)\n",
        "  danger_count = guard.metrics[\"events\"][\"danger_rows\"]\n",
        "\n",
        "  if danger_count == 0:\n",
        "    return df.select(\"user_id\", explode(\"events\").alias(\"event\"))\n",
        "\n",
        "  else:\n",
        "    print(\"\\n--Applying Skew Mitigation Strategy--\\n\")\n",
        "    # Strategy: ISOLATE -> REPARTITION -> EXPLODE -> UNION\n",
        "\n",
        "    # 1. Split the Stream\n",
        "    # Small rows go to the fast lane.\n",
        "    small_df = df_checked.filter(col(\"_array_size\") <= 10000)\n",
        "\n",
        "    # Huge rows go to the heavy lifting lane.\n",
        "    huge_df = df_checked.filter(col(\"_array_size\") > 10000)\n",
        "\n",
        "    # 2. Process Small Data (Standard Explode)\n",
        "    small_exploded = small_df.select(\"user_id\", explode(\"events\").alias(\"event\"))\n",
        "\n",
        "    # 3. Process Huge Data (The \"Salted\" Fix)\n",
        "    # Why OOM happens: Spark puts the one huge row on ONE core.\n",
        "    # Fix: We force Spark to scatter this one user across 100 partitions\n",
        "    # *before* exploding.\n",
        "\n",
        "    # We use repartition(N) to force a shuffle.\n",
        "    # Since we only have a few huge rows, we can just round-robin repartition.\n",
        "    huge_exploded = (huge_df\n",
        "                      .repartition(100) # <--- THE MAGIC FIX\n",
        "                      .select(\"user_id\", explode(\"events\").alias(\"event\")))\n",
        "\n",
        "    # 4. Union back together\n",
        "    final_df = small_exploded.union(huge_exploded)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ==============================================================================\n",
        "# EXECUTION\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NestedArrayOOMFix\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # 1. Generate Dangerous Data\n",
        "    raw_df = generate_skewed_data(spark)\n",
        "\n",
        "    # 2. Run the Protected Pipeline\n",
        "    result_df = safe_explode_pipeline(spark, raw_df)\n",
        "\n",
        "    print(\"\\n--- Final Pipeline Stats ---\")\n",
        "    print(f\"Total Exploded Rows: {result_df.count()}\")\n",
        "\n",
        "    # Verify we processed the killer users\n",
        "    killer_count = result_df.filter(col(\"user_id\").contains(\"KILLER\")).count()\n",
        "    print(f\"Killer User Events Processed: {killer_count}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuabizRYXwYQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}